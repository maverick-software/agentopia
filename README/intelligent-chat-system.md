# Intelligent Chat System - Contextual Awareness & LLM Debugging

This document provides comprehensive information about Agentopia's intelligent chat processing system, including contextual awareness, intent classification, unified context loading, and the LLM Debug Modal.

## 📋 Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Processing Pipeline](#processing-pipeline)
- [Contextual Awareness](#contextual-awareness)
- [Intent Classification](#intent-classification)
- [Unified Context Loading](#unified-context-loading)
- [LLM Debug Modal](#llm-debug-modal)
- [Performance Optimizations](#performance-optimizations)
- [Token Tracking](#token-tracking)
- [System Prompts](#system-prompts)
- [Caching Strategy](#caching-strategy)
- [Debugging & Monitoring](#debugging--monitoring)

## Overview

Agentopia's chat system is a sophisticated multi-stage pipeline that processes user messages with contextual awareness, intelligent tool routing, and comprehensive debugging capabilities.

### Key Features

- **🧠 Contextual Awareness**: Understands what users actually mean, not just what they literally say
- **🎯 Intent Classification**: Determines tool requirements before loading them
- **⚡ Performance Optimized**: Saves 750ms per message for non-tool requests
- **🔍 Complete Visibility**: LLM Debug Modal shows every API call with full details
- **💰 Cost Tracking**: Separate input/output token tracking for accurate cost calculation
- **🗄️ Database-Managed Prompts**: System prompts stored in database for easy updates
- **♻️ Intelligent Caching**: Reduces redundant LLM calls with TTL-based caching

## Architecture

### System Flow

```
┌──────────────────────────────────────────────────────────────────┐
│                         User Message                             │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 1: Contextual Awareness Analysis                          │
│ ─────────────────────────────────────────────────────────────── │
│ • Model: gpt-4o-mini                                            │
│ • Duration: ~2-3 seconds                                        │
│ • Purpose: Understand actual user intent in context            │
│ • Input: User message + conversation summary + recent messages  │
│ • Output: Interpreted meaning, resolved references, confidence  │
│ • Cache: 5-minute TTL, 500 max entries                         │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 2: Intent Classification                                   │
│ ─────────────────────────────────────────────────────────────── │
│ • Model: gpt-4o-mini                                            │
│ • Duration: ~1-2 seconds                                        │
│ • Purpose: Determine if tools are needed                       │
│ • Input: User message + contextual interpretation              │
│ • Output: requiresTools (boolean), confidence, reasoning       │
│ • Cache: 5-minute TTL, 1000 max entries                        │
│ • Performance: Saves 750ms if tools not needed                 │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 3: Unified Context Loading (Async)                        │
│ ─────────────────────────────────────────────────────────────── │
│ • Duration: ~95ms (parallel loading)                            │
│ • Loads: Agent settings, conversation summary, recent messages  │
│ • Output: Single formatted context message                      │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 4: Tool Loading (Conditional)                             │
│ ─────────────────────────────────────────────────────────────── │
│ • Duration: ~750ms                                              │
│ • Condition: Only if Stage 2 determined tools are needed        │
│ • Purpose: Load available tools for agent                       │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 5: Main LLM Call                                          │
│ ─────────────────────────────────────────────────────────────── │
│ • Model: Agent's configured model (e.g., gpt-4)                │
│ • Duration: Variable (depends on response length)              │
│ • Input: Full context + contextual guidance + tools (if any)   │
│ • Output: Agent response + tool calls (if needed)              │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────────┐
│ Stage 6: Response Synthesis                                      │
│ ─────────────────────────────────────────────────────────────── │
│ • Ensures proper markdown formatting                            │
│ • Synthesizes tool results into coherent response              │
│ • Attaches metadata and processing details                     │
└──────────────────────────┬───────────────────────────────────────┘
                           ↓
                   Agent Response
```

### Core Files

**Backend (Edge Functions)**:
- `supabase/functions/chat/processor/handlers.ts` - Main message handler with LLM call tracking
- `supabase/functions/chat/processor/utils/contextual-awareness.ts` - Contextual analysis
- `supabase/functions/chat/processor/utils/intent-classifier.ts` - Tool requirement detection
- `supabase/functions/chat/core/context/unified_context_loader.ts` - Context loading
- `supabase/functions/chat/processor/builder.ts` - Response builder with metrics
- `supabase/functions/chat/processor/stages.ts` - Pipeline stages with metrics propagation
- `supabase/functions/chat/processor/types.ts` - TypeScript interfaces including `ProcessingMetrics`

**Frontend**:
- `src/components/modals/LLMDebugModal.tsx` - Debug modal UI
- `src/components/chat/MessageComponents.tsx` - Message display with Debug button
- `src/pages/AgentChatPage.tsx` - Main chat interface

## Processing Pipeline

### Stage 1: Contextual Awareness

**Purpose**: Understand what the user is ACTUALLY asking for, not just the literal text.

**Process**:
1. Fetch conversation summary from `conversation_summary_boards`
2. Retrieve recent messages (last 5-10) from conversation history
3. Get agent personality/settings for contextual understanding
4. Analyze user message with full context using gpt-4o-mini
5. Return interpreted meaning, resolved references, and confidence

**Example**:
```
User: "Send it to them"
↓
Contextual Analysis:
- Conversation Summary: "User discussing proposal with client John Doe"
- Recent Messages: "I've finished the Q4 proposal"
↓
Interpreted Meaning: "Send the Q4 proposal to client John Doe"
Resolved References:
  - "it" → "Q4 proposal"
  - "them" → "client John Doe"
Confidence: high
```

**Output Structure**:
```typescript
{
  originalMessage: string;
  interpretedMeaning: string;
  userIntent: string;
  contextualFactors: string[];
  confidence: 'high' | 'medium' | 'low';
  resolvedReferences: Record<string, string>;
  suggestedClarifications?: string[];
  analysisTimeMs: number;
  fromCache: boolean;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

### Stage 2: Intent Classification

**Purpose**: Determine if the user's message requires external tools or is just a question/conversation.

**Key Distinction**:
- **Capability Questions**: "Are you able to send emails?" → **No tools needed**
- **Action Requests**: "Send an email to john@example.com" → **Tools needed**

**Process**:
1. Receive contextual interpretation from Stage 1
2. Analyze if message is a capability question or action request
3. Return boolean `requiresTools` with confidence and reasoning

**Examples**:

| User Message | Requires Tools | Reasoning |
|-------------|----------------|-----------|
| "What tools do you have?" | ❌ No | Informational question about capabilities |
| "Can you access Gmail?" | ❌ No | Capability question |
| "Search my Gmail for invoices" | ✅ Yes | Action request requiring Gmail tool |
| "Tell me about backlinks" | ❌ No | Information request |
| "Get backlink data for example.com" | ✅ Yes | Action requiring external tool |

**Output Structure**:
```typescript
{
  requiresTools: boolean;
  confidence: 'high' | 'medium' | 'low';
  detectedIntent: string;
  suggestedTools?: string[];
  reasoning: string;
  classificationTimeMs: number;
  fromCache: boolean;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

### Stage 3: Unified Context Loading

**Purpose**: Load all necessary context (agent settings, summary, history) in parallel for maximum efficiency.

**Components Loaded**:
1. **Agent Settings**: System prompts, personality, behavior settings
2. **Conversation Summary**: Key facts, topics, entities from summary board
3. **Recent Messages**: Last 10-15 messages for immediate context

**Performance**:
- All components loaded in parallel using `Promise.all`
- Total duration: ~95ms (vs 250ms+ sequential)
- Single formatted context message injected early in message array

**Output**:
```
📋 CONVERSATION CONTEXT

Agent Settings:
- Role: Customer Support Specialist
- Personality: Professional, helpful, empathetic

Conversation Summary:
- Key Facts: User is inquiring about SEO services
- Main Topics: backlinks, SEO strategy
- Previous Discussions: Website optimization, keyword research

Recent Messages (last 15):
[User]: "What is SEO?"
[Assistant]: "SEO stands for Search Engine Optimization..."
...
```

### Stage 4: Tool Loading (Conditional)

**Purpose**: Only load tools if Stage 2 determined they're needed.

**Performance Impact**:
- Tool loading takes ~750ms
- Skipped for 60-70% of messages (questions, conversations)
- Significant latency reduction for informational queries

### Stage 5: Main LLM Call

**Purpose**: Generate the actual agent response with full context.

**Contextual Guidance Injection**:
The system automatically injects contextual guidance into the message array:

```
🧠 CONTEXTUAL UNDERSTANDING:
User said: "Tell me about types of backlinks"
Interpreted meaning: "User wants information about the different types of backlinks in the context of SEO"
User's actual intent: Learn about the various categories or classifications of backlinks and their significance in SEO.
Confidence: high

Contextual factors:
  - User previously asked about backlinks in general
  - Conversation is focused on SEO education

Respond to their ACTUAL INTENT (Learn about the various categories or classifications of backlinks and their significance in SEO.), not just the literal message text.
```

This ensures the LLM understands context and responds appropriately.

### Stage 6: Response Synthesis

**Purpose**: Format the response and handle tool results.

**Process**:
1. Ensure proper markdown formatting
2. If tools were called, synthesize results into coherent response
3. Attach metadata including all processing details
4. Return complete message with `processingDetails` containing `llm_calls` array

## Contextual Awareness

### Implementation Details

**File**: `supabase/functions/chat/processor/utils/contextual-awareness.ts`

**Class**: `ContextualAwarenessAnalyzer`

**Key Methods**:
- `analyzeContext()` - Main entry point for contextual analysis
- `performContextualAnalysis()` - Executes LLM call with system prompt
- `buildContextPrompt()` - Constructs prompt with full context
- `getSystemPrompt()` - Fetches prompt from database with caching

**System Prompt**:
- Stored in `system_prompts` table with key `'contextual_awareness'`
- Cached for 5 minutes after fetching
- Fallback to hardcoded prompt if database fetch fails
- Can be updated without code changes

**Caching Strategy**:
- Cache key: Hash of (user message + conversation ID + agent ID)
- TTL: 5 minutes
- Max entries: 500
- LRU eviction when full

**Usage Tracking**:
```typescript
{
  prompt_tokens: 245,      // Input to gpt-4o-mini
  completion_tokens: 89,   // Output from gpt-4o-mini
  total_tokens: 334        // Sum of both
}
```

## Intent Classification

### Implementation Details

**File**: `supabase/functions/chat/processor/utils/intent-classifier.ts`

**Class**: `IntentClassifier`

**Key Methods**:
- `classifyIntent()` - Main entry point
- `performClassification()` - Executes LLM call
- `getSystemPrompt()` - Fetches prompt from database with caching

**Decision Matrix**:

The classifier uses explicit rules to distinguish capability questions from actions:

```
Capability Questions (No Tools):
- "Are you able to..."
- "Can you..."
- "Do you have access to..."
- "What tools do you have?"
- "What can you do?"

Action Requests (Tools Required):
- "Send..."
- "Get..."
- "Search..."
- "Find..."
- "Create..."
- "Delete..."
- "Update..."
```

**System Prompt**:
- Stored in `system_prompts` table with key `'intent_classifier'`
- Includes explicit examples of capability vs action
- Recently updated (Oct 22, 2025) with clearer distinction

**Caching Strategy**:
- Cache key: Hash of (user message + agent ID)
- TTL: 5 minutes
- Max entries: 1000
- LRU eviction when full

## Unified Context Loading

### Implementation Details

**File**: `supabase/functions/chat/core/context/unified_context_loader.ts`

**Class**: `UnifiedContextLoader`

**Method**: `loadContext()`

**Parallel Operations**:
```typescript
const [agentSettings, summaryInfo, recentMessages] = await Promise.all([
  this.fetchAgentSettings(agentId),
  this.fetchConversationSummary(conversationId),
  this.fetchRecentMessages(conversationId)
]);
```

**Output Format**:
```typescript
{
  contextMessage: string;      // Formatted context for LLM
  tokensSaved: number;         // Estimated token savings
  loadTimeMs: number;          // Total load duration
  sources: {
    agentSettings: boolean;
    conversationSummary: boolean;
    recentMessages: number;
  };
}
```

## LLM Debug Modal

### Overview

The LLM Debug Modal provides complete visibility into every LLM call made during message processing.

### Features

1. **Complete Request/Response Visibility**
   - Full request payload (model, messages, tools, parameters)
   - Full response data (text, tool calls, usage stats)
   
2. **Token Tracking**
   - Input tokens (↓ blue)
   - Output tokens (↑ green)
   - Total tokens (Σ purple)
   
3. **Stage Breakdown**
   - #1 🧠 Contextual Awareness Analysis
   - #2 🎯 Intent Classification
   - #3 💬 Main LLM Call
   
4. **Performance Metrics**
   - Duration for each stage (ms)
   - Timestamp for each call
   - Total time across all stages
   
5. **Developer Tools**
   - Copy request/response to clipboard
   - Expandable JSON views
   - Syntax-highlighted code blocks

### Usage

**Access**: Click the purple "Debug" button (</> icon) next to any assistant message

**UI Layout**:
```
┌─────────────────────────────────────────────────────────────┐
│ LLM Debug Viewer                                       [X]   │
├─────────────────────────────────────────────────────────────┤
│ Total Stages: 3  │  Total Tokens: 2,336 (↓1,435 / ↑901)    │
│                  │  Total Time: 11,452ms                    │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│ ▶ #1 🧠 Contextual Awareness Analysis   ↓245 ↑89  2683ms   │
│                                                              │
│ ▶ #2 🎯 Intent Classification           ↓156 ↑45  1277ms   │
│                                                              │
│ ▼ #3 💬 Main LLM Call                   ↓1034 ↑767 7492ms  │
│ ┌────────────────────────────────────────────────────────┐  │
│ │ 📤 Request                                   [Copy]    │  │
│ │ ┌────────────────────────────────────────────────────┐ │  │
│ │ │ {                                                  │ │  │
│ │ │   "model": "gpt-4",                               │ │  │
│ │ │   "messages": [...],                              │ │  │
│ │ │   "tools": [...],                                 │ │  │
│ │ │   "temperature": 0.7,                             │ │  │
│ │ │   "max_tokens": 1200                              │ │  │
│ │ │ }                                                  │ │  │
│ │ └────────────────────────────────────────────────────┘ │  │
│ │                                                        │  │
│ │ ✨ Response                                   [Copy]   │  │
│ │ ┌────────────────────────────────────────────────────┐ │  │
│ │ │ {                                                  │ │  │
│ │ │   "text": "Here is information about...",         │ │  │
│ │ │   "tool_calls": [],                               │ │  │
│ │ │   "usage": {                                      │ │  │
│ │ │     "prompt_tokens": 1034,                        │ │  │
│ │ │     "completion_tokens": 767,                     │ │  │
│ │ │     "total_tokens": 1801                          │ │  │
│ │ │   }                                                │ │  │
│ │ │ }                                                  │ │  │
│ │ └────────────────────────────────────────────────────┘ │  │
│ │                                                        │  │
│ │ Token Usage:                                          │  │
│ │ ↓ Input: 1034 tokens  ↑ Output: 767 tokens           │  │
│ │ Σ Total: 1801 tokens                                  │  │
│ └────────────────────────────────────────────────────────┘  │
│                                                              │
├─────────────────────────────────────────────────────────────┤
│ 💡 Tip: Click any stage to expand                [Expand All]│
└─────────────────────────────────────────────────────────────┘
```

### Implementation

**File**: `src/components/modals/LLMDebugModal.tsx`

**Data Source**: `processingDetails.llm_calls` array from message metadata

**LLM Call Structure**:
```typescript
{
  stage: string;              // 'contextual_awareness', 'intent_classification', 'main_llm_call'
  description: string;        // '🧠 Contextual Awareness Analysis'
  request: {
    model: string;            // 'gpt-4o-mini', 'gpt-4'
    messages: any[];          // Full message array
    tools: any[];             // Available tools (if any)
    temperature: number;      // LLM temperature
    max_tokens: number;       // Token limit
    [key: string]: any;       // Other parameters
  };
  response: {
    text?: string;            // Response text
    tool_calls?: any[];       // Tool calls made
    usage: {
      prompt_tokens: number;
      completion_tokens: number;
      total_tokens: number;
    };
    [key: string]: any;       // Other response data
  };
  timestamp: string;          // ISO timestamp
  duration_ms: number;        // Execution time
}
```

## Performance Optimizations

### 1. Conditional Tool Loading

**Savings**: 750ms per message

**How It Works**:
- Intent Classification (Stage 2) determines if tools needed
- If `requiresTools: false`, skip tool loading entirely
- Applies to 60-70% of messages (questions, conversations)

**Example**:
```
User: "What is SEO?"
→ Intent Classification: requiresTools: false
→ Skip tool loading (750ms saved!)
→ Direct to Main LLM Call
→ Total time reduced by ~40%
```

### 2. Parallel Context Loading

**Savings**: ~155ms per message (250ms sequential → 95ms parallel)

**How It Works**:
- Agent settings, summary, and history loaded simultaneously
- Uses `Promise.all` for parallel execution
- Single formatted context message

### 3. Intelligent Caching

**Savings**: ~2-4 seconds for cache hits

**Contextual Awareness Cache**:
- TTL: 5 minutes
- Max entries: 500
- Key: Hash(message + conversation_id + agent_id)

**Intent Classification Cache**:
- TTL: 5 minutes
- Max entries: 1000
- Key: Hash(message + agent_id)

**Cache Hit Rates**:
- Contextual Awareness: ~15-25% (varies by conversation)
- Intent Classification: ~30-40% (common patterns)

### 4. Database-Stored Prompts

**Benefits**:
- No code deployment for prompt updates
- Cached for 5 minutes (reduces DB queries)
- Fallback to hardcoded prompts (high availability)

**System Prompts Table**:
```sql
CREATE TABLE system_prompts (
  id UUID PRIMARY KEY,
  key TEXT UNIQUE NOT NULL,
  name TEXT NOT NULL,
  content TEXT NOT NULL,
  category TEXT,
  is_active BOOLEAN DEFAULT true,
  version INTEGER DEFAULT 1,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Keys**:
- `contextual_awareness` - Contextual analysis prompt
- `intent_classifier` - Tool requirement detection prompt

## Token Tracking

### Input vs Output Tokens

**Why Track Separately?**
- Different pricing: Input tokens often cheaper than output
- Cost analysis: Know what's being sent vs generated
- Optimization: Identify stages with high token usage

**Display Format**:

**Header (collapsed)**:
```
↓ 245  ↑ 89
```

**Stats Bar**:
```
Total Tokens: 2,336 (↓1,435 / ↑901)
```

**Expanded Stage**:
```
↓ Input:  1,034 tokens
↑ Output:   767 tokens
Σ Total:  1,801 tokens
```

### Token Usage by Stage

**Typical Breakdown**:
```
Stage 1 (Contextual Awareness):
  Input:  200-300 tokens (prompt + context)
  Output:  50-100 tokens (interpretation)
  
Stage 2 (Intent Classification):
  Input:  150-200 tokens (prompt + message)
  Output:  30-50 tokens (classification)
  
Stage 3 (Main LLM Call):
  Input:  800-2000 tokens (full context + history)
  Output: 200-1000 tokens (agent response)
  
Total: ~1,500-3,500 tokens per message
```

### Cost Calculation

**Example Pricing** (as of Oct 2025):
- GPT-4o-mini: $0.15 / 1M input tokens, $0.60 / 1M output tokens
- GPT-4: $30 / 1M input tokens, $60 / 1M output tokens

**Per-Message Cost**:
```
Contextual Awareness (gpt-4o-mini):
  Input:  250 tokens × $0.15 / 1M = $0.0000375
  Output:  75 tokens × $0.60 / 1M = $0.0000450
  Subtotal: $0.0000825

Intent Classification (gpt-4o-mini):
  Input:  175 tokens × $0.15 / 1M = $0.00002625
  Output:  40 tokens × $0.60 / 1M = $0.0000240
  Subtotal: $0.00005025

Main LLM Call (gpt-4):
  Input:  1,200 tokens × $30 / 1M = $0.036
  Output:   600 tokens × $60 / 1M = $0.036
  Subtotal: $0.072

Total per message: ~$0.072 ($72 per 1,000 messages)
```

## System Prompts

### Database Management

**Advantages**:
- ✅ Update prompts without code deployment
- ✅ Version control for prompts
- ✅ A/B testing capabilities
- ✅ Rollback to previous versions
- ✅ Cached for performance (5-min TTL)

**Table Structure**:
```sql
system_prompts
  - id: UUID (primary key)
  - key: TEXT UNIQUE (e.g., 'contextual_awareness')
  - name: TEXT (display name)
  - content: TEXT (actual prompt)
  - category: TEXT (e.g., 'memory', 'classification')
  - is_active: BOOLEAN (enable/disable)
  - version: INTEGER
  - created_at: TIMESTAMPTZ
  - updated_at: TIMESTAMPTZ
```

### Current Prompts

**1. Contextual Awareness** (`contextual_awareness`)
- **Category**: memory
- **Purpose**: Analyze user messages in conversation context
- **Updated**: October 20, 2025
- **Version**: 1

**2. Intent Classifier** (`intent_classifier`)
- **Category**: classification
- **Purpose**: Determine if tools are needed
- **Updated**: October 22, 2025
- **Version**: 2 (updated with capability vs action distinction)

### Updating Prompts

**Via SQL**:
```sql
UPDATE system_prompts
SET content = 'Your new prompt here...',
    version = version + 1,
    updated_at = NOW()
WHERE key = 'contextual_awareness'
  AND is_active = true;
```

**Via Supabase Dashboard**:
1. Go to Table Editor
2. Select `system_prompts`
3. Edit the desired row
4. Update `content` field
5. Increment `version`
6. Save changes

**Cache Invalidation**:
- Automatic after 5 minutes
- Or restart Edge Functions to clear immediately

## Caching Strategy

### Why Caching?

**Benefits**:
- Reduces redundant LLM calls
- Saves API costs
- Improves response time
- Handles similar/repeated queries efficiently

### Cache Implementation

**Contextual Awareness Cache**:
```typescript
private interpretationCache = new Map<string, {
  interpretation: ContextualInterpretation;
  timestamp: number;
}>();

private readonly CACHE_TTL = 5 * 60 * 1000; // 5 minutes
private readonly MAX_CACHE_SIZE = 500;

// Cache key generation
private getCacheKey(
  userMessage: string,
  conversationId: string,
  agentId: string
): string {
  return `${conversationId}:${agentId}:${this.hashMessage(userMessage)}`;
}
```

**Intent Classification Cache**:
```typescript
private classificationCache = new Map<string, CachedClassification>();

private readonly CACHE_TTL = 5 * 60 * 1000; // 5 minutes
private readonly MAX_CACHE_SIZE = 1000;

// Cache key generation
private getCacheKey(message: string, agentId: string): string {
  return `${agentId}:${this.hashMessage(message)}`;
}
```

### Cache Eviction

**LRU (Least Recently Used)**:
- When cache reaches MAX_SIZE, oldest entries evicted
- TTL-based expiration: Entries older than 5 minutes auto-removed
- Proactive cleanup on each cache check

### Cache Performance

**Hit Rate Monitoring**:
```typescript
{
  fromCache: boolean,  // Indicates if result came from cache
  analysisTimeMs: number  // Near-zero if cached
}
```

**Expected Hit Rates**:
- Contextual Awareness: 15-25%
- Intent Classification: 30-40%

**Cache Savings**:
- Contextual Awareness cache hit: ~2-3 seconds saved
- Intent Classification cache hit: ~1-2 seconds saved
- Combined: Up to 5 seconds saved per cached message

## Debugging & Monitoring

### Development Debugging

**Console Logs** (Edge Functions):
```typescript
// Stage 1
console.log('[ContextualAwareness] 🔍 Analyzing contextual meaning...');
console.log('[ContextualAwareness] ✅ Interpretation complete');

// Stage 2
console.log('[IntentClassifier] Classification result:', classification);
console.log('[IntentClassifier] ⚡ Skipped tool loading (~750ms saved)');

// Metrics
console.log('[TextMessageHandler] 📊 Building metrics with llmCalls:', {
  llmCallsCount: llmCalls.length,
  stages: llmCalls.map(c => c.stage),
});

console.log('[buildSuccessResponse] 🔍 metrics.llm_calls:', {
  exists: !!metrics.llm_calls,
  count: metrics.llm_calls?.length || 0,
});
```

**LLM Debug Modal**:
- Available on every assistant message
- Shows complete pipeline execution
- No need to check server logs for most issues

### Production Monitoring

**Key Metrics to Track**:
1. **Average Response Time**: Should be 3-8 seconds
2. **Tool Loading Skips**: Should be 60-70% of messages
3. **Cache Hit Rates**: 
   - Contextual: 15-25%
   - Intent: 30-40%
4. **Token Usage**: 1,500-3,500 tokens per message
5. **Error Rates**: < 1% per stage

**Supabase Dashboard**:
- Edge Functions → `chat` → Logs
- Filter by log level (info, warn, error)
- Search for specific stages or IDs

### Troubleshooting

**Problem**: Slow response times

**Check**:
1. Are tools being loaded unnecessarily? (Intent Classification)
2. Is caching working? (Look for `fromCache: true`)
3. Is context loading taking too long? (Should be ~95ms)

**Problem**: Incorrect intent classification

**Check**:
1. Review system prompt in `system_prompts` table
2. Check `intent_classifier` version
3. Look at reasoning in Debug Modal
4. Consider updating prompt with examples

**Problem**: Missing contextual understanding

**Check**:
1. Is conversation summary being populated?
2. Are recent messages being fetched?
3. Review contextual interpretation in Debug Modal
4. Check `contextual_awareness` system prompt

**Problem**: High token usage

**Check**:
1. Review token breakdown in Debug Modal
2. Identify stages with unusually high usage
3. Consider shortening context or history
4. Check for overly verbose system prompts

## Best Practices

### For Prompt Engineering

1. **Test prompts in Debug Modal** before deploying
2. **Version control** your prompts (increment version number)
3. **Include explicit examples** in classification prompts
4. **Keep prompts focused** on single responsibility
5. **Monitor cache hit rates** - too specific prompts reduce cache effectiveness

### For Performance

1. **Let Intent Classification work** - don't force tool loading
2. **Keep conversation summaries concise** - reduces token usage
3. **Limit recent messages** to 10-15 for context
4. **Monitor token usage per stage** - optimize high-usage stages

### For Debugging

1. **Always check Debug Modal first** before diving into logs
2. **Copy request/response JSON** for detailed analysis
3. **Track token usage trends** - identify costly patterns
4. **Review confidence scores** - low confidence = potential issues

### For Cost Optimization

1. **Use caching effectively** - similar queries benefit most
2. **Intent Classification saves money** - fewer tool loads = lower costs
3. **Track input vs output tokens** - optimize both separately
4. **Consider gpt-4o-mini** for classification tasks - 10x cheaper

---

## Summary

Agentopia's Intelligent Chat System represents a sophisticated, production-ready implementation of contextual AI conversation processing. The multi-stage pipeline ensures agents understand context, make intelligent decisions about tool usage, and provide complete visibility into LLM operations.

**Key Achievements**:
- ✅ 750ms average latency reduction per message
- ✅ 60-70% reduction in unnecessary tool loads
- ✅ Complete LLM call visibility for debugging
- ✅ Separate input/output token tracking for cost analysis
- ✅ Database-managed prompts for easy updates
- ✅ Intelligent caching with 15-40% hit rates
- ✅ Production-ready with comprehensive monitoring

**Future Enhancements**:
- Multi-turn tool execution planning
- Streaming support for contextual awareness
- Advanced caching strategies (semantic similarity)
- Real-time token cost tracking
- A/B testing for system prompts

