# LLM Debug Modal - Implementation Complete

**Date:** October 20, 2025  
**Status:** âœ… **IMPLEMENTED AND READY**  
**Feature:** LLM Debug Viewer for Chat Processing Pipeline

---

## ğŸ¯ **Purpose**

Provide developers and power users with visibility into **every LLM call** made during a conversation turn, including:
- Request parameters (model, messages, tools, temperature)
- Response content (text, tool calls, usage stats)
- Timing information (duration per stage)
- Token usage per stage

This enables:
- âœ… Debugging AI responses
- âœ… Optimizing prompts
- âœ… Understanding processing flow
- âœ… Monitoring token usage
- âœ… Identifying bottlenecks

---

## ğŸ—ï¸ **Architecture**

### **Button Location:**
Next to the "Process" button on assistant messages in the chat interface.

### **Modal Features:**
1. **Expandable Stages** - Each LLM call is a collapsible section
2. **Request/Response Viewer** - JSON formatted with syntax highlighting
3. **Copy to Clipboard** - One-click copy for each request/response
4. **Token Usage Stats** - Per-stage and total token counts
5. **Duration Tracking** - Time spent on each stage
6. **Pretty Formatting** - Color-coded JSON for readability

---

## ğŸ“ **Files Created/Modified**

### **1. LLMDebugModal Component (NEW)**
**File:** `src/components/modals/LLMDebugModal.tsx`

**Features:**
- âœ… Expandable stage viewer
- âœ… JSON syntax highlighting (green for requests, blue for responses)
- âœ… Copy to clipboard with visual feedback
- âœ… Stats bar showing total stages/tokens/time
- âœ… "Expand All" button
- âœ… Responsive design with max-width

**Props:**
```typescript
interface LLMDebugModalProps {
  isOpen: boolean;
  onClose: () => void;
  processingDetails: any; // Contains LLM call data
}
```

**Data Structure:**
```typescript
interface LLMCall {
  stage: string;                    // e.g., 'contextual_awareness', 'intent_classification'
  description: string;              // e.g., 'ğŸ§  Contextual Awareness Analysis'
  request: {
    model?: string;
    messages?: any[];
    tools?: any[];
    temperature?: number;
    max_tokens?: number;
  };
  response: {
    content?: string;
    tool_calls?: any[];
    usage?: {
      prompt_tokens?: number;
      completion_tokens?: number;
      total_tokens?: number;
    };
  };
  timestamp?: string;
  duration_ms?: number;
}
```

---

### **2. MessageList Component (UPDATED)**
**File:** `src/components/chat/MessageComponents.tsx`

**Changes:**
- âœ… Added `onShowDebugModal` prop to `MessageListProps`
- âœ… Added purple "Debug" button next to "Process" button
- âœ… Button only shows when `message.metadata?.processingDetails` exists
- âœ… Hover effect: purple glow on hover
- âœ… Code icon (</> symbol)

**Code:**
```tsx
{/* Debug Button - Show LLM calls and responses */}
{message.metadata?.processingDetails && onShowDebugModal && (
  <button
    onClick={() => {
      console.log('[MessageList] Debug button clicked for message:', message);
      onShowDebugModal(message.metadata?.processingDetails);
    }}
    className="flex items-center space-x-1 cursor-pointer hover:bg-purple-500/20 rounded-md px-1.5 py-0.5 transition-colors"
    title="View LLM requests and responses"
  >
    <svg className="h-3 w-3 text-purple-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
    </svg>
    <span className="text-xs text-purple-500">Debug</span>
  </button>
)}
```

---

### **3. AgentChatPage (UPDATED)**
**File:** `src/pages/AgentChatPage.tsx`

**Changes:**
- âœ… Added `showDebugModal` state
- âœ… Added `debugProcessingDetails` state
- âœ… Imported `LLMDebugModal` component
- âœ… Passed `onShowDebugModal` to `MessageList`
- âœ… Rendered `LLMDebugModal` at bottom of component

**Code:**
```tsx
// State
const [showDebugModal, setShowDebugModal] = useState(false);
const [debugProcessingDetails, setDebugProcessingDetails] = useState<any>(null);

// MessageList prop
onShowDebugModal={(details) => {
  console.log('[AgentChatPage] Debug modal opened with details:', details);
  setDebugProcessingDetails(details);
  setShowDebugModal(true);
}}

// Modal render
<LLMDebugModal
  isOpen={showDebugModal}
  onClose={() => setShowDebugModal(false)}
  processingDetails={debugProcessingDetails}
/>
```

---

## ğŸ”„ **Processing Stages Displayed**

The modal displays LLM calls from these stages:

### **1. ğŸ§  Contextual Awareness Analysis**
- Model: `gpt-4o-mini`
- Purpose: Interpret user message in conversation context
- Shows: Resolved references, interpreted meaning, user intent

### **2. ğŸ¯ Intent Classification**
- Model: `gpt-4o-mini`
- Purpose: Determine if tools are needed
- Shows: Classification result, confidence, reasoning

### **3. ğŸ’¬ Main LLM Call**
- Model: `gpt-4` (or configured model)
- Purpose: Generate response or tool calls
- Shows: Full message history, tool schemas, response content

### **4. ğŸ”§ Tool Execution**
- Purpose: Execute tools (MCP, integrations)
- Shows: Tool name, parameters, result, errors

### **5. ğŸ”„ Retry Analysis** (if retries occurred)
- Model: `gpt-4o-mini`
- Purpose: Analyze tool failure and suggest fixes
- Shows: Error analysis, suggested parameter fixes

---

## ğŸ¨ **UI Design**

### **Color Scheme:**
- **Purple** - Debug theme color
- **Green** - Request JSON (terminal-style)
- **Blue** - Response JSON
- **Yellow** - Sparkles icon (responses)
- **Dark background** - Code blocks with syntax highlighting

### **Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”§ LLM Debug Viewer                            [X]  â”‚
â”‚  View all LLM requests and responses...              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ Total Stages: 5  |  ğŸ”¥ Tokens: 2,340  |  â± 1,250msâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  â–¶ #1 ğŸ§  Contextual Awareness Analysis    342ms  â”‚
â”‚  â–¶ #2 ğŸ¯ Intent Classification             123ms  â”‚
â”‚  â–¼ #3 ğŸ’¬ Main LLM Call                     785ms  â”‚
â”‚     â”œâ”€ ğŸ“¤ Request                          [Copy]  â”‚
â”‚     â”‚   {                                           â”‚
â”‚     â”‚     "model": "gpt-4",                         â”‚
â”‚     â”‚     "messages": [...]                         â”‚
â”‚     â”‚   }                                           â”‚
â”‚     â”‚                                               â”‚
â”‚     â””â”€ ğŸ“¥ Response                         [Copy]  â”‚
â”‚         {                                           â”‚
â”‚           "content": "Here is...",                  â”‚
â”‚           "usage": {...}                            â”‚
â”‚         }                                           â”‚
â”‚                                                     â”‚
â”‚  â–¶ #4 ğŸ”§ Tool: zapier_search_contact        89ms  â”‚
â”‚                                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ’¡ Tip: Click any stage to expand...  [Expand All] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š **Example: What Users Will See**

### **Contextual Awareness Stage:**
```json
{
  "model": "gpt-4o-mini",
  "messages": [
    {
      "role": "system",
      "content": "You are a contextual awareness analyzer..."
    },
    {
      "role": "user",
      "content": "AGENT PROFILE:\nName: Jordan's Agent\n\nCURRENT USER MESSAGE:\n\"Send him an email\""
    }
  ],
  "temperature": 0.3,
  "max_tokens": 500
}

// Response:
{
  "content": "{\n  \"interpretedMeaning\": \"Send an email to John Doe\",\n  \"userIntent\": \"Compose and send email\",\n  \"resolvedReferences\": {\n    \"him\": \"John Doe\"\n  }\n}",
  "usage": {
    "prompt_tokens": 245,
    "completion_tokens": 87,
    "total_tokens": 332
  }
}
```

### **Main LLM Call Stage:**
```json
{
  "model": "gpt-4",
  "messages": [
    { "role": "system", "content": "You are Jordan's Agent..." },
    { "role": "assistant", "content": "=== CONTEXT WINDOW ===" },
    { "role": "system", "content": "ğŸ§  CONTEXTUAL UNDERSTANDING: User said: \"Send him an email\" But actually means: \"Send email to John Doe\"" },
    { "role": "user", "content": "Send him an email" }
  ],
  "tools": [
    { "name": "gmail_send_email", "description": "Send an email via Gmail", "parameters": {...} }
  ],
  "temperature": 0.7,
  "max_tokens": 1200
}

// Response:
{
  "content": null,
  "tool_calls": [
    {
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "gmail_send_email",
        "arguments": "{\"to\":\"john.doe@example.com\",\"subject\":\"Hello\",\"body\":\"Hi John!\"}"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 1,234,
    "completion_tokens": 156,
    "total_tokens": 1,390
  }
}
```

---

## ğŸš€ **Usage Flow**

### **For Users:**
1. Send a message to an agent
2. Agent responds
3. See "Process" and "Debug" buttons next to the response
4. Click **"Debug"** button
5. Modal opens showing all LLM stages
6. Click any stage to expand and view details
7. Click "Copy" to copy request/response JSON
8. Click "Expand All" to see everything at once

### **For Developers:**
- Use to debug why agent gave a certain response
- Optimize prompts by seeing what's sent to LLM
- Monitor token usage per stage
- Identify which stage is slowest
- Copy exact requests to test in OpenAI playground

---

## âœ… **Benefits**

| Before | After |
|--------|-------|
| âŒ No visibility into LLM calls | âœ… Full transparency of all stages |
| âŒ Can't see why agent responded that way | âœ… See exact prompts and responses |
| âŒ Hard to debug tool selection issues | âœ… See tool schemas sent to LLM |
| âŒ No token usage breakdown | âœ… Per-stage token usage visible |
| âŒ Can't optimize prompts | âœ… Copy exact requests to test |

---

## ğŸ¯ **Future Enhancements**

Potential additions (not implemented yet):
- [ ] Search/filter stages
- [ ] Export all data as JSON file
- [ ] Compare two different responses side-by-side
- [ ] Show diff between retry attempts
- [ ] Performance graphs (time per stage)
- [ ] Cost calculation (tokens Ã— pricing)
- [ ] Save favorite debugging sessions

---

## ğŸ“ **Testing Checklist**

- [x] Button appears next to Process button
- [x] Button only shows when processingDetails exists
- [x] Modal opens when Debug button clicked
- [x] All stages display correctly
- [x] Expand/collapse works
- [x] Copy to clipboard works
- [x] Token usage displays correctly
- [x] Duration displays correctly
- [x] "Expand All" button works
- [x] Close button works
- [x] Responsive on mobile
- [x] No linting errors

---

## ğŸ‰ **Summary**

**The LLM Debug Modal is now LIVE!** ğŸš€

Users can now:
- âœ… See **every LLM call** made during chat processing
- âœ… View **full requests and responses** with syntax highlighting
- âœ… **Copy JSON** to clipboard for testing
- âœ… Track **token usage** and **timing** per stage
- âœ… **Debug AI behavior** by seeing exact prompts
- âœ… **Optimize performance** by identifying slow stages

**This provides unprecedented visibility into the AI pipeline!** ğŸ”âœ¨

---

**Implementation Complete:** October 20, 2025  
**Ready for Production:** âœ… YES  
**Location:** Next to "Process" button on agent messages  
**Icon:** Purple `</>` code symbol

